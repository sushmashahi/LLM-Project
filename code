# LLM-Project
import streamlit as st
from langchain_openai import OpenAI  # Correct import for OpenAI
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory  # Use ConversationBufferMemory for managing conversation history
from langchain.chains import LLMChain

# Set up OpenAI LLM with the required temperature
llm = OpenAI(temperature=0.7)

# Set up memory to store the conversation history
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# Define the prompt template
prompt_template = """You are a helpful assistant. The user asked: {query}"""
prompt = PromptTemplate(input_variables=["query"], template=prompt_template)

# Set up the LLMChain with the prompt and memory
chain = LLMChain(prompt=prompt, llm=llm, memory=memory)

# Streamlit interface
st.title('Chatbot with LangChain')
user_input = st.text_input("Ask a question:")

if user_input:
    # Get the response from the chain using the user input
    response = chain.run(query=user_input)
    
    # Display the response
    st.write("Response:", response)
